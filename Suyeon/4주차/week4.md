### 💡 가상메모리의 역할은 무엇인가요?

- 가상메모리란?

= Virtual Memory System

![image](https://github.com/CS-Entrance/Chapter1/assets/89267864/2bc52109-43bb-4337-a69f-93e6ad7e3d0f)

OS에서 실제 물리 메모리(RAM)보다 더 큰 주소 공간을 프로세스에게 제공하기 위해 사용된다

어떤 프로세스가 실행될 때 메모리에 프로세스의 전체를 올리지 않더라도 실행이 가능하다는 점에서 착안하게 된 메모리 기법이다

이로 인해 여러 개의 프로세스를 처리할 수 있게 된다

메인 메모리의 크기가 한정되어 있으므로 물리적인 메모리 크기보다 크기카 큰 프로세스를 실행시킬 수 없다

예시로 100MB인 메인 메모리에 200MB 크기의 프로세스를 실행할 수 없는 것이다

이러한 문제를 해결하기 위해 메인 메모리를 늘리는 것 뿐만 아니라 더 효율적인 해결 방안이 “가상메모리” 인 것이다

프로세스의 모든 코드는 항상 사용되어야 하거나 필요하지 않다 (ex. 오류처리)

따라서, 프로세스의 필요한 부분만 메인 메모리에 올림으로써 메인 메모리에 올라가는 프로세스의 크기를 줄일 수 있다

그리고 `가상 메모리`와 우리가 앞에서 알아봤던 `페이징` 기법을 함께 사용한다면 `**요구 페이징**` 기법이 탄생하게 되는 것이다

⇒ 프로세스에 페이징 기법을 적용하고 나누어진 페이지들을 필요한 부분과 불필요한 부분으로 나눌 수 있다. 

여기서 필요한 부분만 메모리에 적재를 하면 많은 메모리 낭비를 막고 필요한 프로세스를 한정된 메인 메모리에서 실행시킬 수 있게 되는 것이다.

### 💡 Page Fault에 대해 설명하시오.

간단하게 말하자면 CPU가 요구하는 논리 주소를 통해 Page Table에 접근했지만 **해당 페이지가 메모리에 없는 (=Valid 필드가 0인 경우) 를 말한다**

그렇다면 위에서 설명한 backing store에 해당 페이지가 존재하겠다만 이를 찾기 위해 CPU는 하는 일을 멈추고 OS가 backing store에 IO작업을 통해 필요한 페이지를 가져와 메모리에 적재해야 하는 과정이 필요하다 + valid 필드를 1으로 변경 ⇒ 그럼 CPU가 쉬게 된다 ⇒ 그럼 효율성이 낮아진다

즉! Page Fault를 많이 줄여야 된다!

### 💡 Demand Paging(요구 페이징)에 대해 설명하시오.

![image](https://github.com/CS-Entrance/Chapter1/assets/89267864/aea1bab0-7084-47aa-8305-2e1933558e23)

위에서 말했듯 페이징 기법을 통한 메모리 관리 전략중 하나가 요구 페이징이다

요구되는(필요한) 페이지만 메모리에 올린다는 의미로 요구 페이징이라고 부른다

이 기법으로 메모리 낭비를 줄일 수 있고 필요한 페이지만 읽어올 수 있으므로 시간이 단축된다

- backing store
    - swap device로 HW 부분이다
    - 프로세스의 불필요한 페이지를 임시로 보관하는 공간이다

MMU의 재배치 레지스터를 통해 논리 주소를 물리 주소로 바꾸어주는 “주소 변환 과정”과

동시에 CPU에겐 프로세스가 하나의 연속된 메모리 공간처럼 보여야 한다

따라서, backing store가 추가되면서 페이징 기법에 필요한 ***Page Table에 backing store에 적재 되어 있는지 (메모리에 적재되어 있는지) 구분할 수 있는 값이 필요***해졌다

그래서 `Valid` 필드를 Page Table에 추가하고 1과 0의 값으로 메모리에 적재되어 있는지 없는지를 구분할 수 있다

요구페이징에도 2가지 종류가 있다

1️⃣ **pure demand paging**

처음부터 모든 페이지를 적재시키지 않고 CPU가 요구할 때만 valid를 바꾸어 페이지를 적재하는 방법

- 페이지가 요구할 때만 메모리에 적재하므로 메모리 낭비는 줄일 수 있다
- 하지만 요구에 의한 Page Fault를 처리하기 위해선 효율이 떨어진다

2️⃣ **prepaging**

필요한 페이지를 우선 적재시키고 필요할 때 다른 페이지를 추가로 적재시키는 방법

- 미리 올라와있어 처리 속도는 빠르지만 잘못 사용하면 메모리가 낭비될 수 있다

### 💡 Swapping이 무엇인가요?

![image](https://github.com/CS-Entrance/Chapter1/assets/89267864/200913b1-f40e-4244-b04e-5613485f44f5)

요구 페이징을 설명하는 이미지에 있는 swap out과 swap in이 swapping 방식이다

즉, 필요한 프로세스의 페이지를 메모리에 부르기 위해선 swap in, 불필요한 페이지를 backing store에 내보내기 위해선 swap out을 사용한다

swapping과 demand paging(요구 페이징)의 관계

공통점 → 둘 다 backing store와 메인 메모리를 상호 이동한다

차이점 → 

SWAPPING - 프로세스 “**전체**”가 backing store에서 이동

DEMAND PAGING - 프로세스의 “**페이지**”가 backing store에서 이동

### 💡 페이지 적중율을 극대화 시키기 위한 방법에는 무엇이 있는지 간략히 설명해주세요.

![image](https://github.com/CS-Entrance/Chapter1/assets/89267864/26729192-192d-484c-b40c-855105a35d14)

가장 유용하게 사용되는 방법으론 `지역성`이 있다. 

지역성(Locality)도 공간 지역성, 시간 지역성으로 나눌 수 있다

1️⃣ 시간 지역성

> 한 번 읽었던 코드를 다시 읽을 확률이 높다
while문, for문등 같은 구간을 반복하는 명령이 이에 해당한다
> 
- 최근에 참조된 주소 내용은 곧 다음에 다시 참조된다는 시간 지역성

2️⃣ 공간 지역성

> 코드를 읽을 때 현재 코드의 주변에 있는 코드를 읽을 확률이 높다
Page Fault가 발생하여 디스크에서 페이지를 가져올 때 주변 코드들을 block 단위로 가져오게 되면 주변 코드를 읽을 확률이 높으니 다음의 Page Fault 확률이 낮아지게 된다
> 
- 한 번 참조된 메모리 옆에 있는 메모리를 참조할 확률이 높다는 공간 지역성
- “**순차적 지역성**”
    - 공간 지역성의 일종
    - 프로그램이 메모리 상에서 순차적으로 실행될 때 인접한 메모리 주소의 데이터나 명령어가 참조될 가능성이 높아지는 현상

### 💡 Cache 메모리를 사용하는 이유에 대해 설명하시오.


![image](https://github.com/CS-Entrance/Chapter1/assets/89267864/41f39b24-50aa-4c28-b34d-cce1e6879f38)

![image](https://github.com/CS-Entrance/Chapter1/assets/89267864/6ff4488a-9ec6-4df3-8d11-185ed45e3176)


**캐시 메모리**란 중앙 처리 장치(CPU)의 처리속도와 주 기억장치(RAM)의 접근 속도 차이를 극복하고 성능을 향상시키기 위해 사용하는 범용 메모리이다

즉, CPU와 RAM 두 장치간의 속도차이에 따른 **병목 현상**을 줄이고 성능을 향상 시키는 것이 캐시 메모리의 목적이다

캐시의 종류는 위 이미지와 같이 L1, L2, L3가 있다. L은 Level을 의미한다

- L1 캐시
    - 일반적으로 코어안에 내장되어 데이터 사용, 참조에 가장 먼저 사용된다
    - 8~64KB의 용량으로 CPU가 가장 빠르게 접근한다
- L2 캐시
    - L1 캐시에서 데이터를 찾지 못한 경우 L2 캐시에서 탐색한다
    - L1 캐시보다 속도가 느리고 64KB~4MB의 용량으로 L1보다 상대적으로 큰 용량을 가진다
- L3 캐시
    - 동일한 원리로 동작하지만 웬만한 프로세서에선 L3 캐시 메모리까진 사용하지 않는다
    - L2 캐시에서 충분히 해결할 수 있기 때문이다

✨ 병목현상

> 어떤 시스템 내 데이터의 집중적인 사용으로 인해 전체 시스템에 절대적 영향을 미치는 부분의 사용 빈도가 증가해 그 부분의 성능이 저하되어 전체 시스템이 마비되는 현상
> 

이를 위해, 입출력 데이터를 **버퍼**링하여 메모리 접근 없이 **빠른 입출력**을 제공한다

하지만, 모든 곳에 캐시 메모리를 사용한다면 좋겠지만 **용량이 작고 비싸다**는 단점이 있어 한계가 존재한다

최근에 사용된 데이터나 자주 사용되는 데이터를 임시로 저장해두는데 이는 대부분의 프로그램이 한번 사용한 데이터를 다시 사용할 가능성이 높으며 (시간 지역성) 그 주변의 데이터도 곧 사용될 가능성이 높기 (공간 지역성) 때문이다

![image](https://github.com/CS-Entrance/Chapter1/assets/89267864/aa4041a0-d008-4daa-94ea-eeed9580c077)

캐시가 등장하게 된 배경에는 **파레토의 법칙**이 존재한다

80퍼센트의 결과는 20퍼센트의 원인으로 발생한다는 법칙이다

즉, 이것이 캐시가 효율적일 수 있는 이유가 되는 것이다

모든 결과를 캐싱 할 필요는 없으며 많이 사용되는 20%만 캐싱한다면 전체적인 성능을 향상시킬 수 있다는 것이다!

그렇다면 캐시 또한 지역성을 사용해 원하는 데이터를 미리 예측하여 불러오는데 이를 성공하는 비율을 `캐시 적중률(Hit rate)`라고 한다

원하는 정보가 캐시 메모리에 있다 → 적중(Hit)

원하는 정보가 캐시 메모리에 존재하지 않는다 → 실패

또한 캐시에 있는 데이터들을 효율적으로 탐색하기 위해 **`캐싱 라인(Caching line)`**을 사용할 수 있다

프로세스는 다양한 주소에 있는 데이터를 사용하기 때문에 자주 사용되는 데이터 주소 또한 연속적이지 않다

따라서 캐시에 저장하는 데이터에는 데이터의 메모리 주소등을 기록해둔 태그를 달아둔다

이러한 태그들의 묶음을 캐싱 라인이라고 부른다

그리고 캐시 메모리에서 데이터를 가져올 때도 캐싱 라인을 참조한다

캐싱 라인의 종류는 아래와 같이 크게 3가지가 있다

1. Full Associate
2. Set Associate
3. Direct Map

![image](https://github.com/CS-Entrance/Chapter1/assets/89267864/57682fb7-5525-41b4-aae0-d78940d5c200)

위 사진은 [메모리 계층 구조]를 나타낸다

아래로 갈수록 데이터 저장하는 공간은 커지지만 속도가 느려지는 메모리임을 알 수 있다

즉, 데이터를 저장하는 공간의 속도와 용량은 반비례 관계이다

속도가 빠르면 저장 공간이 작고 속도가 느리지만 저장 공간은 크다는 특징이 있다

Q. 속도도 빠르고 저장 공간도 큰 메모리는 만들 수 없을까??

A. `YES` 못 만든다. 비용이 너무 비싸다

⇒ 그래서 해당 문제를 해결하기 위해 **메모리 계층 구조**를 만들어 해결하는 것이다

### 추가) 버퍼(Buffer) 메모리란?

버퍼메모리도 캐시메모리와 마찬가지로 **두 장치간의 속도 차이를 극복**하기 위해 사용된다

즉, 버퍼 메모리는 일시적인 데이터 저장소로 이용하는 기억 장치이다

**캐시** → 작업 속도를 빠르게 한다

**버퍼** → 속도가 빠른 쪽에서 느린 쪽으로 갈 때 데이터 손실이 나지않게 한다

**캐시** → CPU가 놀고있는 시간을 줄인다

**버퍼** → 주변기기가 놀고있는 시간을 줄인다

**캐시** → 데이터에 직접적인 접근이 가능하다

**버퍼** → 순차적으로 데이터를 접근한다
